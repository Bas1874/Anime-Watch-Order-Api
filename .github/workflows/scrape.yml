# .github/workflows/scrape.yml (Updated Version)
name: Scrape Reddit Wiki and Generate API File

on:
  schedule:
    - cron: '0 6 * * *' # Runs daily at 6:00 AM UTC
  workflow_dispatch: # Allows manual runs

jobs:
  scrape-and-deploy:
    runs-on: ubuntu-latest
    steps:
      # Step 1: Checkout the repository where the workflow is running.
      - name: Checkout Repo
        uses: actions/checkout@v4

      # Step 2: Set up the Python environment.
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      # Step 3: Install necessary Python libraries.
      # We now need beautifulsoup4 for HTML parsing and lxml as its parser.
      - name: Install dependencies
        run: pip install requests beautifulsoup4 lxml

      # Step 4: Run the scraper script.
      # It will now create a 'data' directory and place two files in it.
      - name: Run Scraper
        env:
          REDDIT_CLIENT_ID: ${{ secrets.REDDIT_CLIENT_ID }}
          REDDIT_CLIENT_SECRET: ${{ secrets.REDDIT_CLIENT_SECRET }}
          REDDIT_USERNAME: ${{ secrets.REDDIT_USERNAME }}
          REDDIT_PASSWORD: ${{ secrets.REDDIT_PASSWORD }}
        run: python scrape_wiki.py ./data

      # Step 5: Commit and push the generated files back to this repository.
      # The action now works on the current repo by default.
      - name: Commit and Push to Repo
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "Automated: Updated watch order data files"
          # This pattern will commit any changes inside the 'data' directory.
          file_pattern: data/*.json
