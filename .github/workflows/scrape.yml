# .github/workflows/scrape.yml
name: Scrape Reddit Wiki and Push to Data Repo

on:
  schedule:
    - cron: '0 6 * * *' # Runs daily at 6:00 AM UTC
  workflow_dispatch: # Allows manual runs

jobs:
  scrape-and-deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Scraper Repo
        uses: actions/checkout@v4

      - name: Checkout Data Repo
        uses: actions/checkout@v4
        with:
          repository: Bas1874/Watch-Order-Seanime # <-- IMPORTANT: Change this
          path: data-repo
          ssh-key: ${{ secrets.DEPLOY_KEY }}

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: Install dependencies
        run: pip install requests

      - name: Run Scraper
        # This 'env' block securely passes your secrets to the Python script
        env:
          REDDIT_CLIENT_ID: ${{ secrets.REDDIT_CLIENT_ID }}
          REDDIT_CLIENT_SECRET: ${{ secrets.REDDIT_CLIENT_SECRET }}
          REDDIT_USERNAME: ${{ secrets.REDDIT_USERNAME }}
          REDDIT_PASSWORD: ${{ secrets.REDDIT_PASSWORD }}
        run: python scrape_wiki.py ./data-repo/watch_order.json

      - name: Commit and Push to Data Repo
        run: |
          cd data-repo
          git config --global user.name "GitHub Action"
          git config --global user.email "action@github.com"
          if ! git diff --quiet; then
            git add watch_order.json
            git commit -m "Automated: Updated watch order data"
            git push
            echo "Changes pushed to data repository."
          else
            echo "No changes to commit."
          fi
